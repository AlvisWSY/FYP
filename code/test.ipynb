{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data.dataset import random_split\n",
    "import numpy as np\n",
    "import os,copy,csv,importlib,time,math\n",
    "import math,random,shutil\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from scipy import signal\n",
    "import pickle as pickle\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.cluster import KMeans, MeanShift, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from itertools import cycle\n",
    "from matplotlib import cm\n",
    "from scipy.stats import gaussian_kde\n",
    "colors = cycle(\"bgrcmykbgrcmykbgrcmykbgrcmyk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "#set the seed for random environment\n",
    "#here we set the seed to 42\n",
    "#########################################################################################\n",
    "def seed_everything(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "#when import the module the seed is set\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class WaveFormDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        \"\"\"\n",
    "        root_dir: 数据集的根目录\n",
    "        \"\"\"\n",
    "        self.samples = []\n",
    "        self.labels = []\n",
    "\n",
    "        # 自动发现数据和标签\n",
    "        for label in ['noise', 'internal', 'corona']:\n",
    "            data_dir = os.path.join(root_dir, label, 'WaveForm')\n",
    "            for file in os.listdir(data_dir):\n",
    "                file_path = os.path.join(data_dir, file)\n",
    "                if file.endswith('.csv'):\n",
    "                    # 读取CSV文件，跳过表头\n",
    "                    data = pd.read_csv(file_path, header=None, skiprows=1, sep='\\t').values\n",
    "                    # 对于文件中的每一行，保存为一个独立的样本\n",
    "                    for row in data:\n",
    "                        self.samples.append(row.astype(np.float32))\n",
    "                        self.labels.append(label)\n",
    "        \n",
    "        # 将文本标签转换为整数\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.labels = self.label_encoder.fit_transform(self.labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 返回单个样本和标签\n",
    "        sample = self.samples[idx]\n",
    "        label = self.labels[idx]\n",
    "        return sample, label\n",
    "\n",
    "# 使用示例\n",
    "root_dir = '/media/mldadmin/home/s123mdg34_04/WangShengyuan/FYP/data' # 根据实际路径调整\n",
    "WaveForm = WaveFormDataset(root_dir=root_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设 dataset 是你的完整数据集\n",
    "dataset_size = len(WaveForm)\n",
    "train_size = int(dataset_size * 0.7) # 70% 数据用于训练\n",
    "val_size = int(dataset_size * 0.15) # 15% 数据用于验证\n",
    "test_size = dataset_size - train_size - val_size # 剩余15%数据用于测试\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(WaveForm, [train_size, val_size, test_size])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(y_hat):\n",
    "    return torch.max(torch.zeros_like(y_hat), y_hat)\n",
    "\n",
    "def optimize_weights(param, learning_rate):\n",
    "    param_new = param - learning_rate * param.grad\n",
    "    param.data = param_new\n",
    "\n",
    "def train(model, epochs, input_features, labels, loss_function, learning_rate):\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # stack the input features and labels along the first dim\n",
    "        x = torch.stack((input_features,), 0)\n",
    "        y = torch.stack((labels))\n",
    "        # apply model\n",
    "        y_hat = model.forward(x)\n",
    "        \n",
    "        # print(x.shape)\n",
    "        loss = loss_function(y_hat, y)\n",
    "        losses.append(loss.detach().item())\n",
    "        # zero_grad\n",
    "        model.zero_grad()\n",
    "        # perform backward\n",
    "        loss.backward()\n",
    "\n",
    "        # optimize weights\n",
    "        for param in model.parameters():\n",
    "            optimize_weights(param, learning_rate)\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Linear(torch.nn.Module):\n",
    "    def __init__(self,num_in_features, \n",
    "                    num_out_features):\n",
    "        super().__init__()\n",
    "        # use num_in_features and num_out_features \n",
    "        w = torch.FloatTensor(num_in_features, num_out_features)\n",
    "        w = torch.nn.init.uniform_(w, -0.2, 0.2)\n",
    "        b = torch.FloatTensor(1)\n",
    "        b = torch.nn.init.uniform_(b, -0.2, 0.2)\n",
    "        self.w = torch.nn.Parameter(w, requires_grad=True)\n",
    "        self.b = torch.nn.Parameter(b, requires_grad=True)\n",
    "        \n",
    "    def forward(self, x):        \n",
    "        # return self.w.T @ x + self.b\n",
    "        # x: *, 2, 1\n",
    "        # w: 2, 1\n",
    "        return self.w.T @ x  + self.b \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeLayerNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer0_0 = Linear(256, 1)\n",
    "        self.layer0_1 = Linear(256, 1)\n",
    "        self.layer0_2 = Linear(256, 1)\n",
    "        self.layer1_0 = Linear(3, 1)\n",
    "        self.layer1_1 = Linear(3, 1)\n",
    "        self.layer2 = Linear(2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # x: *, 2, 1\n",
    "        y_hat0_l0 = self.layer0_0.forward(x) \n",
    "        y_hat1_l0 = self.layer0_1.forward(x) \n",
    "        y_hat2_l0 = self.layer0_2.forward(x)\n",
    "\n",
    "        \n",
    "        # y_hat0_0: *, 1, 1\n",
    "        # y_hat0_1: *, 1, 1\n",
    "        z_0 = torch.relu(y_hat0_l0) \n",
    "        z_1 = torch.relu(y_hat1_l0)\n",
    "        z_2 = torch.relu(y_hat2_l0)\n",
    "        \n",
    "        \n",
    "        # z_0: *, 1, 1\n",
    "        # z_1: *, 1, 1\n",
    "        \n",
    "        z = torch.cat([z_0, z_1, z_2], dim=-2)\n",
    "\n",
    "        y_hat0_l1 = self.layer1_0.forward(z)\n",
    "        y_hat1_l1 = self.layer1_1.forward(z)\n",
    "        z_3 = torch.relu(y_hat0_l1)\n",
    "        z_4 = torch.relu(y_hat1_l1)\n",
    "        z1= torch.cat([z_3, z_4], dim=-2)\n",
    "        \n",
    "        # z: *, 2, 1\n",
    "        # print(z.shape)\n",
    "        y_hat = self.layer2.forward(z1)\n",
    "\n",
    "\n",
    "\n",
    "        return y_hat\n",
    "\n",
    "def loss_function(y_hat, y):\n",
    "    return torch.mean((y_hat - y)**2)\n",
    "\n",
    "# apply seed for random number generator before the model\n",
    "torch.manual_seed(100)\n",
    "\n",
    "model = ThreeLayerNN()\n",
    "epochs = 1000\n",
    "learning_rate = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "stack(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m----> 2\u001b[0m     losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(data\u001b[38;5;241m.\u001b[39mshape, labels)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[16], line 14\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, epochs, input_features, labels, loss_function, learning_rate)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# stack the input features and labels along the first dim\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack((input_features,), \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# apply model\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(x)\n",
      "\u001b[0;31mTypeError\u001b[0m: stack(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor"
     ]
    }
   ],
   "source": [
    "for data, labels in train_loader:\n",
    "    losses = train(model, epochs, data, labels, loss_function, learning_rate)\n",
    "    print(data.shape, labels)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FYP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
